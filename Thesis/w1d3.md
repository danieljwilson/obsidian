[[modeling]]
[[model fitting]]
[[linear regression]]
[[bootstrapping]]
[[multiple linear regression]]
[[mean squared error]]
[[maximum likelihood]]
[[objective function]]
[[polynomial regression]]
[[bias and variance]]
[[cross validation]]
[[Akaike's Information Criterion]]

#neuromatch 

# Modeling Fitting
### Two guiding principles of science:

1. **Parameter Selection**: Models have parameters. 
	- How can we set them?
	- How to interpret the uncertainty
	
2. **Model Selection**
	- Which models do a better job of explaining reality?

### Two model fitting philosophies:
1. **Models as functions** ([[mean squared error|MSE]]/OLS)
	- Find model with small errors
	- frequentist framework

2. **Models as generators** ([[maximum likelihood|MLE]])
	- Find model that assigns high likelihood to the data
	- Bayesian framework
  
 - Likelihood vs probability
    - $\mathcal{L}(\theta|x, y) = p(y|\theta, x)$
    - $p(y|\theta, x)$ -> "probability of observing the response $y$ given parameter $\theta$ and input $x$"
    - $\mathcal{L}(\theta|x, y)$ -> "likelihood model that parameters $\theta$ produced response $y$ from input $x$"

### Two philosophies for comparing models:
1. Goodness of fit
	- popular in statistics
	- e.g. [[Akaike's Information Criterion|AIC]]	

2. [[cross validation|Cross validation]]
	- popular in machine learning
	- prediction error on held out data

# Take Aways
- All models have **parameters**
	- Today is about learning **how to choose** those parameters
- A [[linear regression|linear model]] can have non-linear inputs
- Total model error is a combination of [[bias and variance]]
- Use [[bootstrapping]] to estimate uncertainty



# Resources
- [Youtube playlist](https://www.youtube.com/playlist?list=PLkBQOLLbi18MqTMnc_mMiuawnczEa8xm9)
- Notebooks
- 