# Info
**Title**: Predicting individual differences in conflict detection and bias susceptibility during reasoning
**Journal**: Thinking & Reasoning
**Year**: 2020
**Authors**: [[Jakub Šrol]], [[Wim De Neys]]

**Link**: [paperpile](chrome-extension://bomfdkbfpdhijjbeoicnfhjbdhncfhig/view.html?mp=Ouu2NhoH)
**Tags**: #paper
**Rating**: 

---
**Connections**:
[[dual-process model]]
[[mindware]]
[[cognitive biases]]
[[heuristics]]
[[individual differences]]
[[conflict detection]]
[[bias susceptibility]]


# Notes
## Summary
One of the key components of the susceptibility to cognitive biases is the ability to **monitor for conflict** that may arise between intuitively cued “heuristic” answers and logical principles.

**Question**: which individual factors drive these differences??

Similar to [[Lawson_AM_2020_Comparing fast thinking and slow thinking]] they assign participants tasks both from standard heuristics and biases literature as well as **no-conflict counterparts**.

- [[hybrid dual process models]]
- group-level analyses indicate that people are quite good at [[conflict detection]] (De Neys & Glumicic, 2008; Franssens & De Neys, 2009)
	- this has led researches to think that detection failures not really an issue in accurate reasoning...
	- However other studies have shown that up to 20% of participants **did not show any signs of successful detection**

- theoretically [[mindware]] = essential component of [[conflict detection]]
- versions of traditional JDM tasks can be
	- conflict
	- no conflict
	- neutral (designed not to cue any heuristic response)
- often previous studies just focused on one particular heuristic
	- not great for determining domain generality of the mechanisms...

**Goal 1**
>predict [[individual differences]] in [[conflict detection]]

**Goal 2**
>examine the role of detection ability and [[mindware]] instantiation in participants' overall accuracy on conflict reasoning problems

**Reasoning Tasks**
- syllogistic reasoning task
- [[base-rate neglect bias]] task
- [[conjunction bias]] task
- Bat and ball problem (based off of [[cognitive reflection test]])

Each task had:
- 4 conflict
- 4 no conflict
- 2 neutral

**Conflict detection indices**
- [[response time]]
- response confidence ([[Likert scale]])
- [[response time]] for the confidence estimate itself

**[[mindware instantiation|Mindware instantiation]]**
- used neutral versions of the reasoning problems as a proxy measure of participants' [[mindware instantiation]] (Frey et al., 2018)
	- These showed poor [[measure reliability]] ([[Cronbach’s α]] = .28)
		- likely caused by ceiling effects
		- could also be caused by the task specific nature of [[mindware instantiation]]

**Individual Difference Predictors**
- Cognitive ability: [[Vienna matrix test]]
- Thinking disposition measures: [[need for cognition scale]]
- [[numeracy|Numeracy]]: [[Berlin numeracy test]] & [[subjective numeracy scale]]
- Cognitive reflection measure: used a composite measure of items from Thomas and Oppenheimer (2016) and Srol (2019) modeled after Frederick's (2005) [[cognitive reflection test]].

- created a **detection group** of subjects who had longer [[reaction time]]s and lower confidence for incorrect conflict than correct no-conflict problems. ==What did this group do on correct conflict problems?==

- interesting that detection on one type of problem generally did not correlate with detection on other problems? Quite specific to task demands it seems...i.e. **not** domain general

**Main Analysis**: Predicting individual differences in detection efficiency
- only [[need for cognition]] was related to the latency component of detection efficiency
	- marginal $p=.048$, which explained $1.4\%$ of variance
- cognitive reflection and [[mindware]] predicted confidence detection efficiency ($p=0.04$ & $p=0.011$ respectively) but also weak...

In terms of just predicting individual differences in overall conflict reasoning accuracy all  variables  in  the  final  model  except  for  ==Need  for  Cognition==  and ==latency  detection efficiency== did  significantly  contribute  to  conflict  reasoning  accuracy...

The  single  best  predictor  of  **both**  conflict  detection  efficiency  and  overall  reasoning  accuracy  was  [[mindware instantiation]].
==You could rewrite this as "The  single  best  predictor  of  **both**  conflict  detection  efficiency  and  overall  reasoning  accuracy  was how well they did on no-conflict versions of the task". Which especially for overall reasoning accuracy seems completely unsurprising.==

They do address this:
>The  link  between  conflict  detection,  mindware instantiation,  and  conflict  reasoning  accuracy may not be that surprising as the three factors are all indexed by very similar tasks, i.e.,  conflict,  no-conflict,  and  neutral  versions  of  the  same  reasoning problems. However, mindware instantiation was also moderately related to all of the standard individual difference predictors.   This   further  supports  the  view  that  the  differences  between  participants  in available mindware are indeed meaningful, even if the variations in neutral reasoning problem accuracy are not large.

## Thoughts
A lot of focus on [[conflict detection]]. Will they address the fact that detection is necessary not sufficient? Still need to decide to expend the mental effort in many cases...

Not totally clear why their detection efficiency index is only being calculated based on incorrect responses? Could we not assume that detection is happening in situations where subjects provide a correct response?? Wouldn't this be detection with relevant [[mindware]], vs detection but without requisite [[mindware]]?

Their main takeaway - the predictive power of [[mindware instantiation]] - feels a tad circular as it is the proxy that we have a measure for and the proxy was arrived at by having the subjects **do the actual tasks** (just a no conflict version). 

# Resources
**Papers**
- Frey et al., 2018: ==neutral tasks==
- 